{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3f0580",
   "metadata": {},
   "source": [
    "# 2022/04/30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d62482",
   "metadata": {},
   "source": [
    "# Python Package Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29daca14",
   "metadata": {},
   "source": [
    "How to ensure all the used libraries are available on the executors.\n",
    "\n",
    "for example, we want to run the Pandas UDF example, we will need the pyarrow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbdbc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 16:17:30 WARN Utils: Your hostname, SuideMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.18.142.168 instead (on interface en0)\n",
      "22/04/30 16:17:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 16:17:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, mean_udf(v)=1.5), Row(id=2, mean_udf(v)=6.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main(spark):\n",
    "    df = spark.createDataFrame(\n",
    "        [(1, 1.0), \n",
    "         (1, 2.0), \n",
    "         (2, 3.0), \n",
    "         (2, 5.0), \n",
    "         (2, 10.0)],\n",
    "        (\"id\", \"v\")\n",
    "    )\n",
    "\n",
    "    @pandas_udf(\"double\")\n",
    "    def mean_udf(v: pd.Series) -> float:\n",
    "        return v.mean()\n",
    "    \n",
    "    print(df.groupby(\"id\").agg(mean_udf(df['v'])).collect())\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(SparkSession.builder.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bb895",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b168f1",
   "metadata": {},
   "source": [
    "just refer to the documentation on how to deploy the environment\n",
    "\n",
    "it's the same as just shipping the conda environment, there's multiple methods.\n",
    "\n",
    "BUT, this should be a devOps job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
