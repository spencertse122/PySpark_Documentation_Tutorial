{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80162145",
   "metadata": {},
   "source": [
    "# 2022/04/30\n",
    "\n",
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd6005",
   "metadata": {},
   "source": [
    "## Apache Arrow in PySpark\n",
    "\n",
    "Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. This currently is most beneficial to Python users that work with Pandas/NumPy data. Its usage is not automatic and might require some minor changes to configuration or code to take full advantage and ensure compatibility. This guide will give a high-level description of how to use Arrow in Spark and highlight any differences when working with Arrow-enabled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00019ea4",
   "metadata": {},
   "source": [
    "## Enavling for conversion to/from Pandas\n",
    "\n",
    "Arrow is available as an optimization when converting a Spark DataFrame to a Pandas DataFrame using the call DataFrame.toPandas() and when creating a Spark DataFrame from a Pandas DataFrame with SparkSession.createDataFrame(). To use Arrow when executing these calls, users need to first set the Spark configuration spark.sql.execution.arrow.pyspark.enabled to true. This is disabled by default.\n",
    "\n",
    "In addition, optimizations enabled by spark.sql.execution.arrow.pyspark.enabled could fallback automatically to non-Arrow optimization implementation if an error occurs before the actual computation within Spark. This can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dab6eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be97fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 16:36:31 WARN Utils: Your hostname, SuideMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.18.142.168 instead (on interface en0)\n",
      "22/04/30 16:36:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 16:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b91a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrae using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791afcd",
   "metadata": {},
   "source": [
    "### CAUTIONS: \n",
    "\n",
    "Even with the Arrow optimization making it Pandas is still heavy load. ANDDDD it will fuck up your little baby computer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
