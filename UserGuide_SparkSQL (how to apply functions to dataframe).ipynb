{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb9ab83",
   "metadata": {},
   "source": [
    "# 2022/04/30\n",
    "\n",
    "# Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#recommended-pandas-and-pyarrow-versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b049ae",
   "metadata": {},
   "source": [
    "## Apache Arrow in PySpark\n",
    "\n",
    "Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. This currently is most beneficial to Python users that work with Pandas/NumPy data. Its usage is not automatic and might require some minor changes to configuration or code to take full advantage and ensure compatibility. This guide will give a high-level description of how to use Arrow in Spark and highlight any differences when working with Arrow-enabled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20eaec8",
   "metadata": {},
   "source": [
    "## Enavling for conversion to/from Pandas\n",
    "\n",
    "Arrow is available as an optimization when converting a Spark DataFrame to a Pandas DataFrame using the call DataFrame.toPandas() and when creating a Spark DataFrame from a Pandas DataFrame with SparkSession.createDataFrame(). To use Arrow when executing these calls, users need to first set the Spark configuration spark.sql.execution.arrow.pyspark.enabled to true. This is disabled by default.\n",
    "\n",
    "In addition, optimizations enabled by spark.sql.execution.arrow.pyspark.enabled could fallback automatically to non-Arrow optimization implementation if an error occurs before the actual computation within Spark. This can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ccd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60376981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/30 16:36:31 WARN Utils: Your hostname, SuideMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.18.142.168 instead (on interface en0)\n",
      "22/04/30 16:36:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/30 16:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb68554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrae using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcfcdd",
   "metadata": {},
   "source": [
    "### CAUTIONS!\n",
    "\n",
    "Even with the Arrow optimization making it Pandas is still heavy load. ANDDDD it will fuck up your little baby computer\n",
    "\n",
    "that's because anytime we convert it to pandas, it will trigger `collect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0367af",
   "metadata": {},
   "source": [
    "## Pandas UDFS (aka Vectorized UDFs)\n",
    "\n",
    "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf() as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "Before Spark 3.0, Pandas UDFs used to be defined with pyspark.sql.functions.PandasUDFType. From Spark 3.0 with Python 3.6+, you can also use Python type hints. Using Python type hints is preferred and using pyspark.sql.functions.PandasUDFType will be deprecated in the future release.\n",
    "\n",
    "Note that the type hint should use pandas.Series in all cases but there is one variant that pandas.DataFrame should be used for its input or output type hint instead when the input or output column is of StructType. The following example shows a Pandas UDF which takes long column, string column and struct column, and outputs a struct column. It requires the function to specify the type hints of pandas.Series and pandas.DataFrame as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1dd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# create a spark dataframe that has three columns including a struct column\n",
    "df = spark.createDataFrame(\n",
    "[[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01590724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|long_col|string_col|       struct_col|\n",
      "+--------+----------+-----------------+\n",
      "|       1|  a string|{a nested string}|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e532929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9b0925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|func(long_col, string_col, struct_col)|\n",
      "+--------------------------------------+\n",
      "|                  {a nested string, 9}|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view = df.select(func(\"long_col\", \"string_col\", \"struct_col\"))\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e325a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66a2b9",
   "metadata": {},
   "source": [
    "## Series to Series\n",
    "\n",
    "The type hint can be expressed as pandas.Series, … -> pandas.Series.\n",
    "\n",
    "By using pandas_udf() with the function having such type hints above, it creates a Pandas UDF where the given function takes one or more pandas.Series and outputs one pandas.Series. The output of the function should always be of the same length as the input. Internally, PySpark will execute a Pandas UDF by splitting columns into batches and calling the function for each batch as a subset of the data, then concatenating the results together.\n",
    "\n",
    "The following example shows how to create this Pandas UDF that computes the product of 2 columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a9fe722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57899709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "275f62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95adbd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636ae79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it also works with Spark dataframe!!!!!\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa02a18",
   "metadata": {},
   "source": [
    "## Iterator of Series to Iterator of Series\n",
    "\n",
    "The type hint can be expressed as Iterator[pandas.Series] -> Iterator[pandas.Series].\n",
    "\n",
    "By using pandas_udf() with the function having such type hints above, it creates a Pandas UDF where the given function takes an iterator of pandas.Series and outputs an iterator of pandas.Series. The length of the entire output from the function should be the same length of the entire input; therefore, it can prefetch the data from the input iterator as long as the lengths are the same. In this case, the created Pandas UDF requires one input column when the Pandas UDF is called. To use multiple input columns, a different type hint is required. See Iterator of Multiple Series to Iterator of Series.\n",
    "\n",
    "It is also useful when the UDF execution requires initializing some states although internally it works identically as Series to Series case. The pseudocode below illustrates the example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23aef3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Psuedo code, will not work\n",
    "\n",
    "# from typing import Iterator\n",
    "\n",
    "# @pandas_udf(\"long\")\n",
    "# def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "#     # do some expensive initialization with a state\n",
    "#     state = very_expensive_initialization()\n",
    "#     for x in iterator:\n",
    "#         # use that state for whole iterator.\n",
    "#         yield calculate_with_state(x, state)\n",
    "    \n",
    "    \n",
    "# df.select(calculate(\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61445c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49c8627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "832ac76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the function and create the UDF\n",
    "\n",
    "@pandas_udf('long')\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in iterator:\n",
    "        yield x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db122c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(plus_one(\"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e94e01",
   "metadata": {},
   "source": [
    "## Iterator of Multiple Series to Iterator of Series\n",
    "\n",
    "The type hint can be expressed as Iterator[Tuple[pandas.Series, ...]] -> Iterator[pandas.Series].\n",
    "\n",
    "By using pandas_udf() with the function having such type hints above, it creates a Pandas UDF where the given function takes an iterator of a tuple of multiple pandas.Series and outputs an iterator of pandas.Series. In this case, the created pandas UDF requires multiple input columns as many as the series in the tuple when the Pandas UDF is called. Otherwise, it has the same characteristics and restrictions as Iterator of Series to Iterator of Series case.\n",
    "\n",
    "The following example shows how to create this Pandas UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8443713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two_cols(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99c9839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, x)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(multiply_two_cols(\"x\", \"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a98d6",
   "metadata": {},
   "source": [
    "## Series to Scalar\n",
    "\n",
    "The type hint can be expressed as pandas.Series, … -> Any.\n",
    "\n",
    "By using pandas_udf() with the function having such type hints above, it creates a Pandas UDF similar to PySpark’s aggregate functions. The given function takes pandas.Series and returns a scalar value. The return type should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64. Any should ideally be a specific scalar type accordingly.\n",
    "\n",
    "This UDF can be also used with GroupedData.agg() and Window. It defines an aggregation from one or more pandas.Series to a scalar value, where each pandas.Series represents a column within the group or window.\n",
    "\n",
    "Note that this type of UDF does not support partial aggregation and all data for a group or window will be loaded into memory. Also, only unbounded window is supported with Grouped aggregate Pandas UDFs currently. The following example shows how to use this type of UDF to compute mean with a group-by and window operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e22870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "        [(1, 1.0), \n",
    "         (1, 2.0), \n",
    "         (2, 3.0), \n",
    "         (2, 5.0), \n",
    "         (2, 10.0)],\n",
    "        (\"id\", \"v\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "785c12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the function and create the UDF\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0423d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mean_udf(v)|\n",
      "+-----------+\n",
      "|        4.2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e6dd450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby with an aggregation function\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb3bb6",
   "metadata": {},
   "source": [
    "Adding a groupby back to the frame in one move\n",
    "\n",
    "THIS IS BEAUTIFUL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ae3ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa55549",
   "metadata": {},
   "source": [
    "# Pandas Function APIs\n",
    "\n",
    "Pandas Function APIs can directly apply a Python native function against the whole DataFrame by using Pandas instances. Internally it works similarly with Pandas UDFs by using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. However, a Pandas Function API behaves as a regular API under PySpark DataFrame instead of Column, and Python type hints in Pandas Functions APIs are optional and do not affect how it works internally at this moment although they might be required in the future.\n",
    "\n",
    "From Spark 3.0, grouped map pandas UDF is now categorized as a separate Pandas Function API, DataFrame.groupby().applyInPandas(). It is still possible to use it with pyspark.sql.functions.PandasUDFType and DataFrame.groupby().apply() as it was; however, it is preferred to use DataFrame.groupby().applyInPandas() directly. Using pyspark.sql.functions.PandasUDFType will be deprecated in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e181631",
   "metadata": {},
   "source": [
    "## Group Map\n",
    "\n",
    "Grouped map operations with Pandas instances are supported by DataFrame.groupby().applyInPandas() which requires a Python function that takes a pandas.DataFrame and return another pandas.DataFrame. It maps each group to each pandas.DataFrame in the Python function.\n",
    "\n",
    "This API implements the “split-apply-combine” pattern which consists of three steps:\n",
    "\n",
    "Split the data into groups by using DataFrame.groupBy().\n",
    "\n",
    "Apply a function on each group. The input and output of the function are both pandas.DataFrame. The input data contains all the rows and columns for each group.\n",
    "\n",
    "Combine the results into a new PySpark DataFrame.\n",
    "\n",
    "To use DataFrame.groupBy().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "A Python function that defines the computation for each group.\n",
    "\n",
    "A StructType object or a string that defines the schema of the output PySpark DataFrame.\n",
    "\n",
    "The column labels of the returned pandas.DataFrame must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. See pandas.DataFrame on how to label columns when constructing a pandas.DataFrame.\n",
    "\n",
    "Note that all data for a group will be loaded into memory before the function is applied. This can lead to out of memory exceptions, especially if the group sizes are skewed. The configuration for maxRecordsPerBatch is not applied on groups and it is up to the user to ensure that the grouped data will fit into the available memory.\n",
    "\n",
    "The following example shows how to use DataFrame.groupby().applyInPandas() to subtract the mean from each value in the group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a5abf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41eb1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(pdf):\n",
    "    #pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05d94d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef55f05",
   "metadata": {},
   "source": [
    "## Co-grouped Map\n",
    "\n",
    "Co-grouped map operations with Pandas instances are supported by DataFrame.groupby().cogroup().applyInPandas() which allows two PySpark DataFrames to be cogrouped by a common key and then a Python function applied to each cogroup. It consists of the following steps:\n",
    "\n",
    "Shuffle the data such that the groups of each dataframe which share a key are cogrouped together.\n",
    "\n",
    "Apply a function to each cogroup. The input of the function is two pandas.DataFrame (with an optional tuple representing the key). The output of the function is a pandas.DataFrame.\n",
    "\n",
    "Combine the pandas.DataFrames from all groups into a new PySpark DataFrame.\n",
    "\n",
    "To use groupBy().cogroup().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "A Python function that defines the computation for each cogroup.\n",
    "\n",
    "A StructType object or a string that defines the schema of the output PySpark DataFrame.\n",
    "\n",
    "The column labels of the returned pandas.DataFrame must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. See pandas.DataFrame. on how to label columns when constructing a pandas.DataFrame.\n",
    "\n",
    "Note that all data for a cogroup will be loaded into memory before the function is applied. This can lead to out of memory exceptions, especially if the group sizes are skewed. The configuration for maxRecordsPerBatch is not applied and it is up to the user to ensure that the cogrouped data will fit into the available memory.\n",
    "\n",
    "The following example shows how to use DataFrame.groupby().cogroup().applyInPandas() to perform an asof join between two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc40ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    time| id| v1|\n",
      "+--------+---+---+\n",
      "|20000101|  1|1.0|\n",
      "|20000101|  2|2.0|\n",
      "|20000102|  1|3.0|\n",
      "|20000102|  2|4.0|\n",
      "+--------+---+---+\n",
      "\n",
      "+--------+---+---+\n",
      "|    time| id| v2|\n",
      "+--------+---+---+\n",
      "|20000101|  1|  x|\n",
      "|20000101|  2|  y|\n",
      "+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5695297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|    time| id| v1| v2|\n",
      "+--------+---+---+---+\n",
      "|20000101|  1|1.0|  x|\n",
      "|20000102|  1|3.0|  x|\n",
      "|20000101|  2|2.0|  y|\n",
      "|20000102|  2|4.0|  y|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c39607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
